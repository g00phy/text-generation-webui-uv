version: "3"
services:
  text-generation-webui-dock:
    container_name: text-generation-webui-dock
    build:
        context: .
        args:
            # Requirements file to use:
            # | GPU | requirements file to use |
            # |--------|---------|
            # | NVIDIA | `requirements.txt` |
            # | AMD | `requirements_amd.txt` |
            # | CPU only | `requirements_cpu_only.txt` |
            # | Apple Intel | `requirements_apple_intel.txt` |
            # | Apple Silicon | `requirements_apple_silicon.txt` |
            # Default: requirements.txt`
            # BUILD_REQUIREMENTS: requirements.txt

            # Extension requirements to build:
            # BUILD_EXTENSIONS:

            # specify which cuda version your card supports: https://developer.nvidia.com/cuda-gpus
            TORCH_CUDA_ARCH_LIST: ${TORCH_CUDA_ARCH_LIST:-7.5}
            BUILD_EXTENSIONS: ${BUILD_EXTENSIONS:-}
            APP_GID: ${APP_GID:-6972}
            APP_UID: ${APP_UID-6972}
    #env_file: .env
    user: "${APP_RUNTIME_UID:-6972}:${APP_RUNTIME_GID:-6972}"
    ports:
        - "${HOST_PORT:-7860}:${CONTAINER_PORT:-7860}"
        - "${HOST_API_PORT:-5000}:${CONTAINER_API_PORT:-5000}"
    stdin_open: true
    tty: true
    volumes:
        - ./models:/app/models  # Mount the models directory from the host
        - ./cache:/app/cache
        - ./characters:/app/characters
        - ./extensions:/app/extensions
        - ./loras:/app/loras
        - ./logs:/app/logs
        - ./models:/app/models
        - ./presets:/app/presets
        - ./prompts:/app/prompts
        - ./softprompts:/app/softprompts
        - ./training:/app/training
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Use count instead of device_ids
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    logging:
      driver: json-file
      options:
        max-file: "3"
        max-size: "10m"
